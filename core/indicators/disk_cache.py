"""
Disk-based caching for indicator results across grid search runs.

Indicators are cached in ~/.cache/trading/indicators/ with keys based on
instrument + indicator_type + params_hash. This enables cross-run reuse.
"""
import hashlib
import json
import pickle
import logging
from pathlib import Path
from typing import Any, Dict, Optional

log = logging.getLogger(__name__)

CACHE_DIR = Path.home() / ".cache" / "trading" / "indicators"


def compute_indicator_cache_key(instrument: str, indicator_type: str, params: Dict[str, Any]) -> str:
    """
    Generate cache key: <instrument>_<indicator_type>_<params_hash>
    
    Params hash ensures cache invalidation when indicator config changes:
    - technical: rsi_period, ema_short_period, ema_long_period, macd_fast, etc.
    - elliott_wave: min_confidence, min_wave_size
    - elliott_wave_inverted: min_confidence_inverted, min_wave_size_inverted
    
    Args:
        instrument: Instrument name (e.g., "sp500", "djia")
        indicator_type: Type of indicator ("technical", "elliott_wave", "elliott_wave_inverted")
        params: Parameter dictionary for the indicator
        
    Returns:
        Cache key string
    """
    # Normalize params (sort keys for stability)
    params_str = json.dumps(params, sort_keys=True, default=str)
    params_hash = hashlib.sha256(params_str.encode()).hexdigest()[:16]
    
    return f"{instrument}_{indicator_type}_{params_hash}"


def get_cached_indicator(cache_key: str) -> Optional[Any]:
    """
    Retrieve cached indicator result.
    
    Args:
        cache_key: Cache key generated by compute_indicator_cache_key()
    
    Returns:
        Cached result or None if not found/corrupted
    """
    cache_file = CACHE_DIR / f"{cache_key}.pkl"
    
    if not cache_file.exists():
        log.debug(f"Cache miss: {cache_key}")
        return None
    
    try:
        with open(cache_file, 'rb') as f:
            result = pickle.load(f)
        log.info(f"Cache hit: {cache_key}")
        return result
    except Exception as e:
        log.warning(f"Cache corrupted: {cache_key}: {e}")
        return None


def save_cached_indicator(cache_key: str, result: Any) -> None:
    """
    Save indicator result to cache.
    
    Args:
        cache_key: Cache key generated by compute_indicator_cache_key()
        result: Indicator result to cache (DataFrame, dict, etc.)
    """
    cache_file = CACHE_DIR / f"{cache_key}.pkl"
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(result, f)
        log.debug(f"Cached: {cache_key}")
    except Exception as e:
        log.warning(f"Failed to cache: {cache_key}: {e}")


def get_cache_stats() -> Dict[str, Any]:
    """
    Get cache statistics: number of files, total size.
    
    Returns:
        Dictionary with cache stats (files, size_mb, cache_dir)
    """
    if not CACHE_DIR.exists():
        return {"files": 0, "size_mb": 0, "cache_dir": str(CACHE_DIR)}
    
    files = list(CACHE_DIR.glob("*.pkl"))
    total_size = sum(f.stat().st_size for f in files)
    
    return {
        "files": len(files),
        "size_mb": total_size / (1024 * 1024),
        "cache_dir": str(CACHE_DIR),
    }


def clear_cache() -> None:
    """Clear all cached indicators."""
    import shutil
    if CACHE_DIR.exists():
        shutil.rmtree(CACHE_DIR)
    log.info(f"Cleared indicator cache: {CACHE_DIR}")
